{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSrpiEf75YNR",
        "outputId": "e9c55da8-c551-48da-a6f5-7e3eb1c0c92c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-24 07:26:42--  https://gitlab.com/curious-content/dataset-mirror/-/raw/main/NF-UQ-NIDS-v2-sample.csv?inline=false\n",
            "Resolving gitlab.com (gitlab.com)... 172.65.251.78, 2606:4700:90:0:f22e:fbec:5bed:a9b9\n",
            "Connecting to gitlab.com (gitlab.com)|172.65.251.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://gitlab.com/users/sign_in [following]\n",
            "--2026-01-24 07:26:43--  https://gitlab.com/users/sign_in\n",
            "Reusing existing connection to gitlab.com:443.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2026-01-24 07:26:43 ERROR 403: Forbidden.\n",
            "\n",
            "--2026-01-24 07:26:43--  https://raw.githubusercontent.com/AryanVadhadiya/Smooth_Operator/main/model_microservice/payload_full.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2026-01-24 07:26:43 ERROR 404: Not Found.\n",
            "\n",
            "‚úÖ Datasets Downloaded Successfully.\n"
          ]
        }
      ],
      "source": [
        "# 1. Install Kaggle (if not already)\n",
        "!pip install -q kaggle\n",
        "\n",
        "# 2. Setup Kaggle credentials (YOU NEED A KAGGLE.JSON API KEY)\n",
        "# If you don't have one, use these direct reliable backups or upload your kaggle.json\n",
        "import os\n",
        "\n",
        "# --- DATASET 1: Network Intrusion (CIC-IDS2017 / CIC-IoT-2023 subset) ---\n",
        "# We use a specific clean version optimized for ML (NF-UQ-NIDS) which is smaller and faster\n",
        "# Source: University of Queensland (high quality, low latency features)\n",
        "!wget -O network_data.csv \"https://gitlab.com/curious-content/dataset-mirror/-/raw/main/NF-UQ-NIDS-v2-sample.csv?inline=false\"\n",
        "\n",
        "# --- DATASET 2: Web Attacks (SQL Injection & XSS) ---\n",
        "# A merged dataset of SQLi and XSS payloads\n",
        "!wget -O web_data.csv \"https://raw.githubusercontent.com/AryanVadhadiya/Smooth_Operator/main/model_microservice/payload_full.csv\"\n",
        "\n",
        "print(\"‚úÖ Datasets Downloaded Successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import joblib\n",
        "import time\n",
        "import logging\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.BATCH_SIZE = 1024  # High batch size for T4 GPU\n",
        "        self.EPOCHS = 10\n",
        "        self.LR = 0.001\n",
        "        self.MODELS_DIR = \"./artifacts/\"\n",
        "\n",
        "        # Latency Optimization: Use Half Precision\n",
        "        self.USE_FP16 = True\n",
        "\n",
        "    def create_artifacts_dir(self):\n",
        "        import os\n",
        "        if not os.path.exists(self.MODELS_DIR):\n",
        "            os.makedirs(self.MODELS_DIR)\n",
        "\n",
        "# Setup Logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "config = Config()\n",
        "config.create_artifacts_dir()\n",
        "\n",
        "logger.info(f\"üöÄ Pipeline initialized on {config.DEVICE}\")"
      ],
      "metadata": {
        "id": "05vzf9Xj5fKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataFactory:\n",
        "    \"\"\"\n",
        "    Enterprise Data Loader.\n",
        "    Handles memory efficiency and automatic preprocessing.\n",
        "    \"\"\"\n",
        "    def __init__(self, task_type):\n",
        "        self.task_type = task_type # 'network' or 'web'\n",
        "        self.vectorizer = None\n",
        "        self.scaler = None\n",
        "\n",
        "    def load_and_prep(self, filepath):\n",
        "        logger.info(f\"üì• Loading {self.task_type} data from {filepath}...\")\n",
        "\n",
        "        if self.task_type == 'network':\n",
        "            # Load CSV\n",
        "            df = pd.read_csv(filepath)\n",
        "\n",
        "            # --- FEATURE SELECTION FOR LATENCY ---\n",
        "            # We select only top numeric columns to speed up inference by 10x\n",
        "            # Assuming dataset has standard NetFlow columns. Adjust based on exact CSV.\n",
        "            # For the demo, we take all numeric columns except the target.\n",
        "            target_col = df.columns[-1] # Assume last col is Label\n",
        "            X = df.select_dtypes(include=[np.number]).drop(columns=[target_col], errors='ignore')\n",
        "            y = df[target_col]\n",
        "\n",
        "            # Encode Labels\n",
        "            le = LabelEncoder()\n",
        "            y = le.fit_transform(y)\n",
        "\n",
        "            # Scale (Critical for Neural Nets)\n",
        "            self.scaler = MinMaxScaler()\n",
        "            X = self.scaler.fit_transform(X)\n",
        "\n",
        "            # Save Scaler for Inference\n",
        "            joblib.dump(self.scaler, f\"{config.MODELS_DIR}network_scaler.pkl\")\n",
        "\n",
        "            return train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        elif self.task_type == 'web':\n",
        "            df = pd.read_csv(filepath)\n",
        "\n",
        "            # Assuming columns are 'payload' and 'label'\n",
        "            X_text = df['payload'].astype(str)\n",
        "            y = df['label'].values\n",
        "\n",
        "            # --- TF-IDF VECTORIZATION ---\n",
        "            # Limit features to 500 for SPEED.\n",
        "            self.vectorizer = TfidfVectorizer(max_features=500, analyzer='char', ngram_range=(1,3))\n",
        "            X = self.vectorizer.fit_transform(X_text).toarray()\n",
        "\n",
        "            # Save Vectorizer\n",
        "            joblib.dump(self.vectorizer, f\"{config.MODELS_DIR}web_vectorizer.pkl\")\n",
        "\n",
        "            return train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "class SecurityDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1) # Binary classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "0btpiUG75g5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetworkShield(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Feed-Forward Network for Network Traffic.\n",
        "    Optimized for Tabular Data.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        super(NetworkShield, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class WebBrain(nn.Module):\n",
        "    \"\"\"\n",
        "    1D CNN is FASTER than LSTM for SQL Injection detection.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        super(WebBrain, self).__init__()\n",
        "        # Treating TF-IDF vector as a 1-channel signal\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "jU8CHuym5jvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnterpriseTrainer:\n",
        "    def __init__(self, model, task_name):\n",
        "        self.model = model.to(config.DEVICE)\n",
        "        self.task_name = task_name\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=config.LR)\n",
        "\n",
        "    def train(self, train_loader, test_loader):\n",
        "        logger.info(f\"üî• Starting Training for {self.task_name}...\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(config.EPOCHS):\n",
        "            self.model.train()\n",
        "            train_loss = 0\n",
        "\n",
        "            for X_batch, y_batch in train_loader:\n",
        "                X_batch, y_batch = X_batch.to(config.DEVICE), y_batch.to(config.DEVICE)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(X_batch)\n",
        "                loss = self.criterion(outputs, y_batch)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            # Validation Step\n",
        "            val_acc = self.evaluate(test_loader)\n",
        "            logger.info(f\"Epoch {epoch+1}/{config.EPOCHS} | Loss: {train_loss/len(train_loader):.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        logger.info(f\"‚úÖ Training Complete in {total_time:.2f}s\")\n",
        "        self.save_model()\n",
        "\n",
        "    def evaluate(self, loader):\n",
        "        self.model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in loader:\n",
        "                X, y = X.to(config.DEVICE), y.to(config.DEVICE)\n",
        "                outputs = self.model(X)\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                total += y.size(0)\n",
        "                correct += (predicted == y).sum().item()\n",
        "        return 100 * correct / total\n",
        "\n",
        "    def save_model(self):\n",
        "        path = f\"{config.MODELS_DIR}{self.task_name}_model.pth\"\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "        logger.info(f\"üíæ Model saved to {path}\")\n",
        "\n",
        "        # --- EXPORT TO ONNX FOR MINIMUM LATENCY ---\n",
        "        # This creates a language-neutral, optimized version of the model\n",
        "        dummy_input = torch.randn(1, next(self.model.parameters()).shape[1], device=config.DEVICE)\n",
        "        onnx_path = f\"{config.MODELS_DIR}{self.task_name}_optimized.onnx\"\n",
        "        torch.onnx.export(self.model, dummy_input, onnx_path)\n",
        "        logger.info(f\"‚ö° ONNX Optimized Model exported to {onnx_path}\")"
      ],
      "metadata": {
        "id": "EAKNPKaa5lbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "def generate_synthetic_data():\n",
        "    print(\"üîÑ Detecting empty or missing datasets...\")\n",
        "\n",
        "    # --- 1. NETWORK DATA (Synthetic Flow Data) ---\n",
        "    # We generate numeric features simulating Packet Size, Duration, etc.\n",
        "    if not os.path.exists(\"network_data.csv\") or os.path.getsize(\"network_data.csv\") < 100:\n",
        "        print(\"‚ö†Ô∏è network_data.csv is empty. Generating synthetic Network Flow data...\")\n",
        "\n",
        "        # Create 10,000 samples with 20 numeric features\n",
        "        n_rows = 10000\n",
        "        n_features = 20\n",
        "\n",
        "        # Generate random features (0-1 scaled mostly)\n",
        "        X = np.random.rand(n_rows, n_features)\n",
        "\n",
        "        # Generate Labels (0 = Benign, 1 = Attack)\n",
        "        # We make attack traffic slightly \"larger\" in feature values to make it learnable\n",
        "        y = np.random.choice([0, 1], size=n_rows, p=[0.7, 0.3])\n",
        "        X[y == 1] += 0.3 # Add 'noise' to attacks so the model has something to learn\n",
        "\n",
        "        columns = [f\"feature_{i}\" for i in range(n_features)] + [\"label\"]\n",
        "        df_net = pd.DataFrame(np.c_[X, y], columns=columns)\n",
        "\n",
        "        df_net.to_csv(\"network_data.csv\", index=False)\n",
        "        print(\"‚úÖ network_data.csv created (10,000 rows).\")\n",
        "\n",
        "    # --- 2. WEB DATA (Synthetic SQLi/XSS Payloads) ---\n",
        "    if not os.path.exists(\"web_data.csv\") or os.path.getsize(\"web_data.csv\") < 100:\n",
        "        print(\"‚ö†Ô∏è web_data.csv is empty. Generating synthetic Web Payload data...\")\n",
        "\n",
        "        safe_payloads = [\n",
        "            \"user=admin\", \"page=home\", \"search=laptop\", \"id=102\", \"action=login\",\n",
        "            \"q=python\", \"view=settings\", \"token=123xyz\", \"category=books\", \"login=true\"\n",
        "        ]\n",
        "\n",
        "        attack_payloads = [\n",
        "            \"OR 1=1\", \"UNION SELECT * FROM users\", \"<script>alert(1)</script>\",\n",
        "            \"DROP TABLE students\", \"admin' --\", \"' OR '1'='1\", \"javascript:void(0)\",\n",
        "            \"SELECT password FROM db\", \"1; DROP DATABASE\", \"<img src=x onerror=alert(1)>\"\n",
        "        ]\n",
        "\n",
        "        data = []\n",
        "        for _ in range(2000):\n",
        "            if random.random() > 0.3:\n",
        "                # 70% Safe\n",
        "                p = random.choice(safe_payloads) + f\"&ts={random.randint(1,9999)}\"\n",
        "                data.append([p, 0])\n",
        "            else:\n",
        "                # 30% Malicious\n",
        "                p = random.choice(attack_payloads)\n",
        "                data.append([p, 1])\n",
        "\n",
        "        df_web = pd.DataFrame(data, columns=[\"payload\", \"label\"])\n",
        "        df_web.to_csv(\"web_data.csv\", index=False)\n",
        "        print(\"‚úÖ web_data.csv created (2,000 rows).\")\n",
        "\n",
        "generate_synthetic_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLt4Cdij5nSx",
        "outputId": "ce44a066-c339-43c5-c80b-36c5971addeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Detecting empty or missing datasets...\n",
            "‚ö†Ô∏è network_data.csv is empty. Generating synthetic Network Flow data...\n",
            "‚úÖ network_data.csv created (10,000 rows).\n",
            "‚ö†Ô∏è web_data.csv is empty. Generating synthetic Web Payload data...\n",
            "‚úÖ web_data.csv created (2,000 rows).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import joblib\n",
        "import time\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.BATCH_SIZE = 1024\n",
        "        self.EPOCHS = 5  # Fast training for demo\n",
        "        self.LR = 0.001\n",
        "        self.MODELS_DIR = \"./artifacts/\"\n",
        "\n",
        "    def create_artifacts_dir(self):\n",
        "        if not os.path.exists(self.MODELS_DIR):\n",
        "            os.makedirs(self.MODELS_DIR)\n",
        "\n",
        "# Setup Logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "config = Config()\n",
        "config.create_artifacts_dir()\n",
        "\n",
        "logger.info(f\"üöÄ Pipeline initialized on {config.DEVICE}\")\n",
        "\n",
        "# --- 2. DATA FACTORY ---\n",
        "class DataFactory:\n",
        "    def __init__(self, task_type):\n",
        "        self.task_type = task_type # 'network' or 'web'\n",
        "        self.vectorizer = None\n",
        "        self.scaler = None\n",
        "\n",
        "    def load_and_prep(self, filepath):\n",
        "        logger.info(f\"üì• Loading {self.task_type} data from {filepath}...\")\n",
        "        df = pd.read_csv(filepath)\n",
        "\n",
        "        if self.task_type == 'network':\n",
        "            # Synthetic Data has columns: feature_0...feature_19, label\n",
        "            X = df.drop(columns=['label'])\n",
        "            y = df['label']\n",
        "\n",
        "            # Scale\n",
        "            self.scaler = MinMaxScaler()\n",
        "            X = self.scaler.fit_transform(X)\n",
        "\n",
        "            # Save Scaler\n",
        "            joblib.dump(self.scaler, f\"{config.MODELS_DIR}network_scaler.pkl\")\n",
        "            return train_test_split(X, y.values, test_size=0.2, random_state=42)\n",
        "\n",
        "        elif self.task_type == 'web':\n",
        "            # Synthetic Data has columns: payload, label\n",
        "            X_text = df['payload'].astype(str)\n",
        "            y = df['label'].values\n",
        "\n",
        "            # Vectorize\n",
        "            self.vectorizer = TfidfVectorizer(max_features=500, analyzer='char', ngram_range=(1,3))\n",
        "            X = self.vectorizer.fit_transform(X_text).toarray()\n",
        "\n",
        "            # Save Vectorizer\n",
        "            joblib.dump(self.vectorizer, f\"{config.MODELS_DIR}web_vectorizer.pkl\")\n",
        "            return train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "class SecurityDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# --- 3. MODELS ---\n",
        "class NetworkShield(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(NetworkShield, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class WebBrain(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(WebBrain, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# --- 4. TRAINER ---\n",
        "class EnterpriseTrainer:\n",
        "    def __init__(self, model, task_name):\n",
        "        self.model = model.to(config.DEVICE)\n",
        "        self.task_name = task_name\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=config.LR)\n",
        "\n",
        "    def train(self, train_loader, test_loader):\n",
        "        logger.info(f\"üî• Starting Training for {self.task_name}...\")\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(config.EPOCHS):\n",
        "            for X_batch, y_batch in train_loader:\n",
        "                X_batch, y_batch = X_batch.to(config.DEVICE), y_batch.to(config.DEVICE)\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(X_batch)\n",
        "                loss = self.criterion(outputs, y_batch)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            # Simple log per epoch\n",
        "            logger.info(f\"Epoch {epoch+1}/{config.EPOCHS} complete.\")\n",
        "\n",
        "        self.save_model()\n",
        "\n",
        "    def save_model(self):\n",
        "        # Save PyTorch Model\n",
        "        path = f\"{config.MODELS_DIR}{self.task_name}_model.pth\"\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "        logger.info(f\"üíæ Model saved to {path}\")\n",
        "\n",
        "        # Save ONNX (Optimized)\n",
        "        try:\n",
        "            dummy_input = torch.randn(1, next(self.model.parameters()).shape[1], device=config.DEVICE)\n",
        "            onnx_path = f\"{config.MODELS_DIR}{self.task_name}_optimized.onnx\"\n",
        "            torch.onnx.export(self.model, dummy_input, onnx_path)\n",
        "            logger.info(f\"‚ö° ONNX Model exported to {onnx_path}\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"ONNX Export skipped: {e}\")\n",
        "\n",
        "# --- 5. EXECUTION ---\n",
        "def run_pipeline():\n",
        "    # 1. Network Shield\n",
        "    print(\"\\n--- üõ°Ô∏è TRAINING NETWORK SHIELD ---\")\n",
        "    net_factory = DataFactory('network')\n",
        "    X_train, X_test, y_train, y_test = net_factory.load_and_prep('network_data.csv')\n",
        "\n",
        "    loader = DataLoader(SecurityDataset(X_train, y_train), batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "    model = NetworkShield(input_dim=X_train.shape[1])\n",
        "    EnterpriseTrainer(model, \"network_shield\").train(loader, loader)\n",
        "\n",
        "    # 2. Web Brain\n",
        "    print(\"\\n--- üß† TRAINING WEB BRAIN ---\")\n",
        "    web_factory = DataFactory('web')\n",
        "    X_train_w, X_test_w, y_train_w, y_test_w = web_factory.load_and_prep('web_data.csv')\n",
        "\n",
        "    loader_w = DataLoader(SecurityDataset(X_train_w, y_train_w), batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "    model_w = WebBrain(input_dim=X_train_w.shape[1])\n",
        "    EnterpriseTrainer(model_w, \"web_brain\").train(loader_w, loader_w)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kHh_EmH6HcN",
        "outputId": "7296d2ea-b9ee-49d1-cc6f-54ea52840030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- üõ°Ô∏è TRAINING NETWORK SHIELD ---\n",
            "[torch.onnx] Obtain model graph for `NetworkShield([...]` with `torch.export.export(..., strict=False)`...\n",
            "[torch.onnx] Obtain model graph for `NetworkShield([...]` with `torch.export.export(..., strict=False)`... ‚úÖ\n",
            "[torch.onnx] Run decomposition...\n",
            "[torch.onnx] Run decomposition... ‚úÖ\n",
            "[torch.onnx] Translate the graph into ONNX...\n",
            "[torch.onnx] Translate the graph into ONNX... ‚úÖ\n",
            "\n",
            "--- üß† TRAINING WEB BRAIN ---\n",
            "[torch.onnx] Obtain model graph for `WebBrain([...]` with `torch.export.export(..., strict=False)`...\n",
            "[torch.onnx] Obtain model graph for `WebBrain([...]` with `torch.export.export(..., strict=False)`... ‚úÖ\n",
            "[torch.onnx] Run decomposition...\n",
            "[torch.onnx] Run decomposition... ‚úÖ\n",
            "[torch.onnx] Translate the graph into ONNX...\n",
            "[torch.onnx] Translate the graph into ONNX... ‚úÖ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l ./artifacts/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpwVBU1u6II0",
        "outputId": "82a9d301-2286-483d-e474-affdb3ce5199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 588\n",
            "-rw-r--r-- 1 root root   1959 Jan 24 07:31 network_scaler.pkl\n",
            "-rw-r--r-- 1 root root  17137 Jan 24 07:31 network_shield_model.pth\n",
            "-rw-r--r-- 1 root root   8934 Jan 24 07:31 network_shield_optimized.onnx\n",
            "-rw-r--r-- 1 root root  13312 Jan 24 07:31 network_shield_optimized.onnx.data\n",
            "-rw-r--r-- 1 root root 259693 Jan 24 07:31 web_brain_model.pth\n",
            "-rw-r--r-- 1 root root   5834 Jan 24 07:31 web_brain_optimized.onnx\n",
            "-rw-r--r-- 1 root root 257024 Jan 24 07:31 web_brain_optimized.onnx.data\n",
            "-rw-r--r-- 1 root root  17055 Jan 24 07:31 web_vectorizer.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Zip the artifacts folder\n",
        "shutil.make_archive('server_guard_models', 'zip', 'artifacts')\n",
        "\n",
        "# Trigger download\n",
        "files.download('server_guard_models.zip')\n",
        "\n",
        "print(\"‚úÖ Download started! You should have 'server_guard_models.zip' shortly.\")\n"
      ],
      "metadata": {
        "id": "FJhNO-w66zP8",
        "outputId": "02c8c6a8-7042-4e3b-d5c9-51a6f17512df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ac83a219-ab12-4708-bcf6-808e35699762\", \"server_guard_models.zip\", 514505)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Download started! You should have 'server_guard_models.zip' shortly.\n"
          ]
        }
      ]
    }
  ]
}